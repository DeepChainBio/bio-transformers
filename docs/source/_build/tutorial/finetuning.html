
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Finetuning &#8212; bio-transformers v0.1.0</title>

  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />


  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">





    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css" />

  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bio-transformers method" href="../api/biotransformers.html" />
    <link rel="prev" title="Embeddings" href="embeddings.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">

    <div class="container-fluid" id="banner"></div>



    <div class="container-xl">
      <div class="row">

<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">

        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">

      <img src="../_static/deepchain-small.png" class="logo" alt="logo">


      <h1 class="site-logo" id="site-title">bio-transformers v0.1.0</h1>

    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started/install.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../getting_started/quick_start.html">
   Quick Start
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../documentation/course.html">
   Getting starting with transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../documentation/logging.html">
   Logging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../documentation/multi_gpus.html">
   Multi-gpus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../documentation/msa.html">
   MSA
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tutorial
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="loglikelihood.html">
   Loglikelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="embeddings.html">
   Embeddings
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Finetuning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Api reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../api/biotransformers.html">
   Bio-transformers method
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../contributing/CONTRIBUTING.html">
   Contributing
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../autoapi/index.html">
   API Reference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../autoapi/biotransformers/index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       biotransformers
      </span>
     </code>
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../autoapi/biotransformers/lightning_utils/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.lightning_utils
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
      <label for="toctree-checkbox-3">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/lightning_utils/data/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.lightning_utils.data
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/lightning_utils/models/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.lightning_utils.models
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/lightning_utils/optimizer/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.lightning_utils.optimizer
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../autoapi/biotransformers/tests/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.tests
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
      <label for="toctree-checkbox-4">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/tests/conftest/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.conftest
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/tests/test_accuracy/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_accuracy
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/tests/test_embeddings/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_embeddings
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/tests/test_logits/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_logits
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/tests/test_loglikelihoods/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_loglikelihoods
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/tests/test_msa/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_msa
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../autoapi/biotransformers/utils/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.utils
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
      <label for="toctree-checkbox-5">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/utils/constant/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.constant
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/utils/deprecated/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.deprecated
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/utils/logger/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.logger
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/utils/msa_utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.msa_utils
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/utils/tqdm_utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.tqdm_utils
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/utils/utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.utils
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../autoapi/biotransformers/wrappers/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.wrappers
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
      <label for="toctree-checkbox-6">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/wrappers/esm_wrappers/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.esm_wrappers
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/wrappers/language_model/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.language_model
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/wrappers/rostlab_wrapper/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.rostlab_wrapper
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../autoapi/biotransformers/wrappers/transformers_wrappers/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.transformers_wrappers
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../autoapi/biotransformers/bio_transformers/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.bio_transformers
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../autoapi/biotransformers/version/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.version
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>






<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">

    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">

            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>


<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->

        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/tutorial/finetuning.md.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/DeepChainBio/bio-transformers"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>

        <a class="edit-button" href="https://github.com/DeepChainBio/bio-transformers/edit/master/docs/tutorial/finetuning.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">

            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-to-finetune-a-model">
   How to finetune a model?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#parameters">
   Parameters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-training-script">
   Example : training script
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example-evaluation-script">
   Example : evaluation script
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">

              <div>

  <div class="tex2jax_ignore mathjax_ignore section" id="finetuning">
<h1>Finetuning<a class="headerlink" href="#finetuning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="how-to-finetune-a-model">
<h2>How to finetune a model?<a class="headerlink" href="#how-to-finetune-a-model" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">bio-transformers</span></code> uses pytorch-lightning to easily load pre-trained model and finetune it on your own datasets. The method <code class="docutils literal notranslate"><span class="pre">finetune</span></code> automatically scale on your visible GPU to train in parallel thanks to the different accelerator.</p>
<p>It is strongly recommended to use the <code class="docutils literal notranslate"><span class="pre">DDP</span></code> accelerator for training : <a class="reference external" href="https://pytorch.org/docs/stable/notes/ddp.html">ddp</a>. You should know that <code class="docutils literal notranslate"><span class="pre">DDP</span></code> will launch several python instances, as a consequence, a model should be finetuned in a separate script, and not be mixed with inference function like <code class="docutils literal notranslate"><span class="pre">compute_loglikelihood</span></code> or <code class="docutils literal notranslate"><span class="pre">compute_embeddings</span></code> to avoid GPU conflicts.</p>
<p>The model will be finetuned randomly by masking a proportion of amino acid in a sequence it commonly does in most state of the art paper. By default, 15% of amino acids will be masked;</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This method is developed to be runned on GPU, please take care to have the proper CUDA installation. Refer to this section for more informations.</p>
</div>
<p>Do not train model <code class="docutils literal notranslate"><span class="pre">DDP</span></code> <strong>accelerator</strong> in a notebook. Do not mix training and compute inference function like <code class="docutils literal notranslate"><span class="pre">compute_accuracy</span></code> or <code class="docutils literal notranslate"><span class="pre">compute_loglikelihood</span></code>  in the same script except with <code class="docutils literal notranslate"><span class="pre">DP</span></code> acceletator.
With <code class="docutils literal notranslate"><span class="pre">DDP</span></code>, load the finetune model in a separate script like below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">biotransformers</span> <span class="kn">import</span> <span class="n">BioTransformers</span>

<span class="n">bio_trans</span> <span class="o">=</span> <span class="n">BioTransformers</span><span class="p">(</span><span class="s2">&quot;esm1_t6_43M_UR50S&quot;</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">bio_trans</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;logs/finetune_masked/version_X/esm1_t6_43M_UR50S_finetuned.pt&quot;</span><span class="p">)</span>
<span class="n">acc_after</span> <span class="o">=</span> <span class="n">bio_trans</span><span class="o">.</span><span class="n">compute_accuracy</span><span class="p">(</span><span class="o">...</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h2>
<p>The function can handle a fasta file or a list of sequences directly:</p>
<ul class="simple">
<li><p><strong>train_sequences</strong>: Could be a list of sequence of a the path of a fasta files with SeqRecords.</p></li>
</ul>
<p>Seven arguments are important for the training:</p>
<ul class="simple">
<li><p><strong>lr</strong>: the default learning rate (keep it low : &lt;5e10-4)</p></li>
<li><p><strong>warmup_updates</strong>:  the number of step (not epochs, optimizer step) to do while increasing the leraning rate from a <strong>warmup_init_lr</strong> to <strong>lr</strong>.</p></li>
<li><p><strong>epochs</strong> :  number of epoch for training. Defaults to 10.</p></li>
<li><p><strong>batch_size</strong> :  This size is only uses internally to compute the <strong>accumulate_grad_batches</strong> for gradient accumulation (TO BE UPDATED). The <strong>toks_per_batch</strong> will dynamically determine the number of sequences in a batch, in order to avoid GPU saturation.</p></li>
<li><p><strong>acc_batch_size</strong> : Number of batch to consider befor computing gradient.</p></li>
</ul>
<p>Three arguments allow to custom the masking function used for building the training dataset:</p>
<ul class="simple">
<li><p><strong>masking_ratio</strong> : ratio of tokens to be masked. Defaults to 0.025.</p></li>
<li><p><strong>random_token_prob</strong> : the probability that the chose token is replaced with a random token.</p></li>
<li><p><strong>masking_prob</strong>: the probability that the chose token is replaced with a mask token.</p></li>
</ul>
<p>All the results will be saved in logs directory:</p>
<ul class="simple">
<li><p><strong>logs_save_dir</strong>: Defaults directory to logs.</p></li>
<li><p><strong>logs_name_exp</strong>: Name of the experience in the logs.</p></li>
<li><p><strong>checkpoint</strong>: Path to a checkpoint file to restore training session.</p></li>
<li><p><strong>save_last_checkpoint</strong>: Save last checkpoint and 2 best trainings models
to restore the training session. Take a large amount of time and memory.</p></li>
</ul>
</div>
<div class="section" id="example-training-script">
<h2>Example : training script<a class="headerlink" href="#example-training-script" title="Permalink to this headline">¶</a></h2>
<p>Training on some swissprot sequences. Training only works on GPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">biodatasets</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">biotransformers</span> <span class="kn">import</span> <span class="n">BioTransformers</span>
<span class="kn">import</span> <span class="nn">ray</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">biodatasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;swissProt&quot;</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_npy_arrays</span><span class="p">(</span><span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sequence&quot;</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Train on small sequence</span>
<span class="n">length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">X</span><span class="p">)))</span> <span class="o">&lt;</span> <span class="mi">200</span>
<span class="n">train_seq</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">length</span><span class="p">][:</span><span class="mi">15000</span><span class="p">]</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">bio_trans</span> <span class="o">=</span> <span class="n">BioTransformers</span><span class="p">(</span><span class="s2">&quot;esm1_t6_43M_UR50S&quot;</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">bio_trans</span><span class="o">.</span><span class="n">finetune</span><span class="p">(</span>
    <span class="n">train_seq</span><span class="p">,</span>
    <span class="n">lr</span><span class="o">=</span><span class="mf">1.0e-5</span><span class="p">,</span>
    <span class="n">warmup_init_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
    <span class="n">toks_per_batch</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">acc_batch_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">warmup_updates</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;ddp&quot;</span><span class="p">,</span>
    <span class="n">checkpoint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">save_last_checkpoint</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="example-evaluation-script">
<h2>Example : evaluation script<a class="headerlink" href="#example-evaluation-script" title="Permalink to this headline">¶</a></h2>
<p>You can easily assees the quality of your finetuning by using the provided function such as <code class="docutils literal notranslate"><span class="pre">compute_accuracy</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">biodatasets</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">biotransformers</span> <span class="kn">import</span> <span class="n">BioTransformers</span>
<span class="kn">import</span> <span class="nn">ray</span>


<span class="n">data</span> <span class="o">=</span> <span class="n">biodatasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;swissProt&quot;</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_npy_arrays</span><span class="p">(</span><span class="n">input_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;sequence&quot;</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Train sequence with length less than 200 AA</span>
<span class="c1"># Test on sequence that was not used for training.</span>
<span class="n">length</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">len</span><span class="p">,</span> <span class="n">X</span><span class="p">)))</span> <span class="o">&lt;</span> <span class="mi">200</span>
<span class="n">train_seq</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">length</span><span class="p">][</span><span class="mi">15000</span><span class="p">:</span><span class="mi">20000</span><span class="p">]</span>

<span class="n">ray</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>
<span class="n">bio_trans</span> <span class="o">=</span> <span class="n">BioTransformers</span><span class="p">(</span><span class="s2">&quot;esm1_t6_43M_UR50S&quot;</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">acc_before</span> <span class="o">=</span> <span class="n">bio_trans</span><span class="o">.</span><span class="n">compute_accuracy</span><span class="p">(</span><span class="n">train_seq</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy before finetuning : </span><span class="si">{</span><span class="n">acc_before</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">Accuracy</span> <span class="n">before</span> <span class="n">finetuning</span> <span class="p">:</span> <span class="mf">0.46</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bio_trans</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;logs/finetune_masked/version_X/esm1_t6_43M_UR50S_finetuned.pt&quot;</span><span class="p">)</span>
<span class="n">acc_after</span> <span class="o">=</span> <span class="n">bio_trans</span><span class="o">.</span><span class="n">compute_accuracy</span><span class="p">(</span><span class="n">train_seq</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy after finetuning : </span><span class="si">{</span><span class="n">acc_after</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">Accuracy</span> <span class="n">before</span> <span class="n">finetuning</span> <span class="p">:</span> <span class="mf">0.76</span>
</pre></div>
</div>
</div>
</div>


              </div>


        <div class='prev-next-bottom'>

    <a class='left-prev' id="prev-link" href="embeddings.html" title="previous page">Embeddings</a>
    <a class='right-next' id="next-link" href="../api/biotransformers.html" title="next page">Bio-transformers method</a>

        </div>

        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>

          By InstaDeep<br/>

            &copy; Copyright 2021, InstaDeep.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>


  </body>
</html>
