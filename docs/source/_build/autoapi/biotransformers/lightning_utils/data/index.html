
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>biotransformers.lightning_utils.data &#8212; bio-transformers v0.1.0</title>

  <link href="../../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />


  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">





    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css" />

  <link rel="preload" as="script" href="../../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">

    <div class="container-fluid" id="banner"></div>



    <div class="container-xl">
      <div class="row">

<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">

        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">

      <img src="../../../../_static/deepchain-small.png" class="logo" alt="logo">


      <h1 class="site-logo" id="site-title">bio-transformers v0.1.0</h1>

    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../getting_started/install.html">
   Installation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../getting_started/quick_start.html">
   Quick Start
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Documentation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../documentation/course.html">
   Getting starting with transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../documentation/logging.html">
   Logging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../documentation/multi_gpus.html">
   Multi-gpus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../documentation/msa.html">
   MSA
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Tutorial
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tutorial/loglikelihood.html">
   Loglikelihood
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tutorial/embeddings.html">
   Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../tutorial/finetuning.html">
   Finetuning
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Api reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../api/biotransformers.html">
   Bio-transformers method
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../contributing/CONTRIBUTING.html">
   Contributing
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../../../index.html">
   API Reference
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="../../index.html">
     <code class="xref py py-mod docutils literal notranslate">
      <span class="pre">
       biotransformers
      </span>
     </code>
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active has-children">
      <a class="reference internal" href="../index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.lightning_utils
        </span>
       </code>
      </a>
      <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
      <label for="toctree-checkbox-3">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul class="current">
       <li class="toctree-l4 current active">
        <a class="current reference internal" href="#">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.lightning_utils.data
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../models/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.lightning_utils.models
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../optimizer/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.lightning_utils.optimizer
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../tests/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.tests
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
      <label for="toctree-checkbox-4">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tests/conftest/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.conftest
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tests/test_accuracy/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_accuracy
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tests/test_embeddings/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_embeddings
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tests/test_logits/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_logits
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tests/test_loglikelihoods/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_loglikelihoods
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../tests/test_msa/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.tests.test_msa
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../utils/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.utils
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
      <label for="toctree-checkbox-5">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../utils/constant/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.constant
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../utils/deprecated/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.deprecated
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../utils/logger/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.logger
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../utils/msa_utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.msa_utils
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../utils/tqdm_utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.tqdm_utils
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../utils/utils/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.utils.utils
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3 has-children">
      <a class="reference internal" href="../../wrappers/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.wrappers
        </span>
       </code>
      </a>
      <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
      <label for="toctree-checkbox-6">
       <i class="fas fa-chevron-down">
       </i>
      </label>
      <ul>
       <li class="toctree-l4">
        <a class="reference internal" href="../../wrappers/esm_wrappers/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.esm_wrappers
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../wrappers/language_model/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.language_model
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../wrappers/rostlab_wrapper/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.rostlab_wrapper
          </span>
         </code>
        </a>
       </li>
       <li class="toctree-l4">
        <a class="reference internal" href="../../wrappers/transformers_wrappers/index.html">
         <code class="xref py py-mod docutils literal notranslate">
          <span class="pre">
           biotransformers.wrappers.transformers_wrappers
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../bio_transformers/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.bio_transformers
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../version/index.html">
       <code class="xref py py-mod docutils literal notranslate">
        <span class="pre">
         biotransformers.version
        </span>
       </code>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>






<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">

    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">

            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>


<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->

        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../../_sources/autoapi/biotransformers/lightning_utils/data/index.rst.txt"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.rst</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/DeepChainBio/bio-transformers"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>

        <a class="edit-button" href="https://github.com/DeepChainBio/bio-transformers/edit/master/docs/autoapi/biotransformers/lightning_utils/data/index.rst"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">

            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-contents">
   Module Contents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classes">
     Classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#functions">
     Functions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.AlphabetDataLoader">
       AlphabetDataLoader
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.AlphabetDataLoader.tok_to_idx">
         tok_to_idx
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.AlphabetDataLoader.tokenizer">
         tokenizer
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.CustomBatchSampler">
       CustomBatchSampler
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.CustomBatchSampler.__iter__">
         __iter__
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.CustomBatchSampler.__len__">
         __len__
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BatchDataset">
       BatchDataset
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BatchDataset.__len__">
         __len__
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BatchDataset.__getitem__">
         __getitem__
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.convert_ckpt_to_statedict">
       convert_ckpt_to_statedict
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.worker_init_fn">
       worker_init_fn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.mask_seq">
       mask_seq
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.collate_fn">
       collate_fn
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data._filter_sequence">
       _filter_sequence
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.get_batch_indices">
       get_batch_indices
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.create_dataloader">
       create_dataloader
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BioDataModule">
       BioDataModule
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data">
         prepare_data
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BioDataModule.setup">
         setup
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BioDataModule.train_dataloader">
         train_dataloader
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BioDataModule.val_dataloader">
         val_dataloader
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#biotransformers.lightning_utils.data.BioDataModule.test_dataloader">
         test_dataloader
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">

              <div>

  <div class="section" id="module-biotransformers.lightning_utils.data">
<span id="biotransformers-lightning-utils-data"></span><h1><a class="reference internal" href="#module-biotransformers.lightning_utils.data" title="biotransformers.lightning_utils.data"><code class="xref py py-mod docutils literal notranslate"><span class="pre">biotransformers.lightning_utils.data</span></code></a><a class="headerlink" href="#module-biotransformers.lightning_utils.data" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this headline">¶</a></h2>
<div class="section" id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.AlphabetDataLoader" title="biotransformers.lightning_utils.data.AlphabetDataLoader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AlphabetDataLoader</span></code></a></p></td>
<td><p>Class that carries tokenizer information</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.CustomBatchSampler" title="biotransformers.lightning_utils.data.CustomBatchSampler"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CustomBatchSampler</span></code></a></p></td>
<td><p>Wraps another sampler to yield a mini-batch of indices.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BatchDataset" title="biotransformers.lightning_utils.data.BatchDataset"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BatchDataset</span></code></a></p></td>
<td><p>An abstract class representing a <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule" title="biotransformers.lightning_utils.data.BioDataModule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BioDataModule</span></code></a></p></td>
<td><p>A DataModule standardizes the training, val, test splits, data preparation and transforms.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="functions">
<h3>Functions<a class="headerlink" href="#functions" title="Permalink to this headline">¶</a></h3>
<table class="longtable table autosummary">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.convert_ckpt_to_statedict" title="biotransformers.lightning_utils.data.convert_ckpt_to_statedict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">convert_ckpt_to_statedict</span></code></a>(checkpoint_state_dict: collections.OrderedDict) → collections.OrderedDict</p></td>
<td><p>This function convert a state_dict coming form pytorch lightning checkpoint to</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.worker_init_fn" title="biotransformers.lightning_utils.data.worker_init_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">worker_init_fn</span></code></a>(worker_id: int)</p></td>
<td><p>Set numpy random seed for each worker.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.mask_seq" title="biotransformers.lightning_utils.data.mask_seq"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mask_seq</span></code></a>(seq: str, tokens: torch.Tensor, prepend_bos: bool, mask_idx: int, pad_idx: int, masking_ratio: float, masking_prob: float, random_token_prob: float, random_token_indices: List[int]) → Tuple[torch.Tensor, torch.Tensor]</p></td>
<td><p>Mask one sequence randomly.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.collate_fn" title="biotransformers.lightning_utils.data.collate_fn"><code class="xref py py-obj docutils literal notranslate"><span class="pre">collate_fn</span></code></a>(samples: Sequence[Tuple[str, str]], tokenizer: esm.data.BatchConverter, alphabet: esm.data.Alphabet, masking_ratio: float, masking_prob: float, random_token_prob: float) → Tuple[torch.Tensor, torch.Tensor]</p></td>
<td><p>Collate function to mask tokens.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data._filter_sequence" title="biotransformers.lightning_utils.data._filter_sequence"><code class="xref py py-obj docutils literal notranslate"><span class="pre">_filter_sequence</span></code></a>(sequences_list: List[str], model: str, filter_len: int) → List[str]</p></td>
<td><p>Function that filter the length of a sequence list</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.get_batch_indices" title="biotransformers.lightning_utils.data.get_batch_indices"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_batch_indices</span></code></a>(sequence_strs, toks_per_batch: int, extra_toks_per_seq: int = 0) → List[List[int]]</p></td>
<td><p>Get the batch idx based on the number of tokens in sequences</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#biotransformers.lightning_utils.data.create_dataloader" title="biotransformers.lightning_utils.data.create_dataloader"><code class="xref py py-obj docutils literal notranslate"><span class="pre">create_dataloader</span></code></a>(sequences: List[str], alphabet: AlphabetDataLoader, filter_len: int, masking_ratio: float, masking_prob: float, random_token_prob: float, num_workers: int = 0, toks_per_batch: int = 128, extra_toks_per_seq: int = 2) → torch.utils.data.DataLoader</p></td>
<td><p>Create the PyTorch Dataset.</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt id="biotransformers.lightning_utils.data.AlphabetDataLoader">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">AlphabetDataLoader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">append_eos</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_dir</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_toks_to_ids</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_tokenizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Callable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.AlphabetDataLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Class that carries tokenizer information</p>
<dl class="py method">
<dt id="biotransformers.lightning_utils.data.AlphabetDataLoader.tok_to_idx">
<code class="sig-name descname"><span class="pre">tok_to_idx</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.AlphabetDataLoader.tok_to_idx" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.AlphabetDataLoader.tokenizer">
<code class="sig-name descname"><span class="pre">tokenizer</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.AlphabetDataLoader.tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return seq-token based on sequence</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="biotransformers.lightning_utils.data.CustomBatchSampler">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">CustomBatchSampler</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.CustomBatchSampler" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.Sampler</span></code></p>
<p>Wraps another sampler to yield a mini-batch of indices.</p>
<p>This custom BatchSampler is inspired from the torch class BatchSampler.
It takes a list of indexes and shuffle the indexes at each epochs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sampler</strong> (<em>List</em>) – List of indexes. indexes are a collections of List[int],</p></li>
<li><p><strong>to the index of the protein sequence.</strong> (<em>corresponding</em>) – </p></li>
<li><p><strong>batch_size</strong> (<em>int</em>) – Size of mini-batch. 1 in our case, a batch are already of correct size.</p></li>
<li><p><strong>drop_last</strong> (<em>bool</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the sampler will drop the last batch if
its size would be less than <code class="docutils literal notranslate"><span class="pre">batch_size</span></code></p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="biotransformers.lightning_utils.data.CustomBatchSampler.__iter__">
<code class="sig-name descname"><span class="pre">__iter__</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.CustomBatchSampler.__iter__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.CustomBatchSampler.__len__">
<code class="sig-name descname"><span class="pre">__len__</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.CustomBatchSampler.__len__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="biotransformers.lightning_utils.data.BatchDataset">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">BatchDataset</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BatchDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.utils.data.Dataset</span></code></p>
<p>An abstract class representing a <code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite <a class="reference internal" href="#biotransformers.lightning_utils.data.BatchDataset.__getitem__" title="biotransformers.lightning_utils.data.BatchDataset.__getitem__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__getitem__()</span></code></a>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
<a class="reference internal" href="#biotransformers.lightning_utils.data.BatchDataset.__len__" title="biotransformers.lightning_utils.data.BatchDataset.__len__"><code class="xref py py-meth docutils literal notranslate"><span class="pre">__len__()</span></code></a>, which is expected to return the size of the dataset by many
<code class="xref py py-class docutils literal notranslate"><span class="pre">Sampler</span></code> implementations and the default options
of <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> by default constructs a index
sampler that yields integral indices.  To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div>
<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BatchDataset.__len__">
<code class="sig-name descname"><span class="pre">__len__</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BatchDataset.__len__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BatchDataset.__getitem__">
<code class="sig-name descname"><span class="pre">__getitem__</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BatchDataset.__getitem__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data.convert_ckpt_to_statedict">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">convert_ckpt_to_statedict</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_state_dict</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">collections.OrderedDict</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">collections.OrderedDict</span><a class="headerlink" href="#biotransformers.lightning_utils.data.convert_ckpt_to_statedict" title="Permalink to this definition">¶</a></dt>
<dd><p>This function convert a state_dict coming form pytorch lightning checkpoint to
a state_dict model that can be load directly in the bio-transformers model.</p>
<p>The keys are updated so that it  m.jionatches those in the bio-transformers</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>checkpoint_state_dict</strong> – a state_dict loaded from a checkpoint</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data.worker_init_fn">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">worker_init_fn</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">worker_id</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.worker_init_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Set numpy random seed for each worker.</p>
<p><a class="reference external" href="https://github.com/pytorch/pytorch/issues/5059#issuecomment-404232359">https://github.com/pytorch/pytorch/issues/5059#issuecomment-404232359</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>worker_id</strong> – unique id for each worker</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data.mask_seq">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">mask_seq</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seq</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokens</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">torch.Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend_bos</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_idx</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_token_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_token_indices</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="headerlink" href="#biotransformers.lightning_utils.data.mask_seq" title="Permalink to this definition">¶</a></dt>
<dd><p>Mask one sequence randomly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seq</strong> – string of the sequence.</p></li>
<li><p><strong>tokens</strong> – tokens corresponding to the sequence, length can be longer than the seq.</p></li>
<li><p><strong>prepend_bos</strong> – if tokenizer adds &lt;bos&gt; token</p></li>
<li><p><strong>mask_idx</strong> – index of the mask token</p></li>
<li><p><strong>pad_idx</strong> – index of the padding token</p></li>
<li><p><strong>masking_ratio</strong> – ratio of tokens to be masked.</p></li>
<li><p><strong>masking_prob</strong> – probability that the chose token is replaced with a mask token.</p></li>
<li><p><strong>random_token_prob</strong> – probability that the chose token is replaced with a random token.</p></li>
<li><p><strong>random_token_indices</strong> – list of token indices that random replacement selects from.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>masked tokens
targets: same length as tokens</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tokens</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data.collate_fn">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">collate_fn</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">samples</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Sequence</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">esm.data.BatchConverter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphabet</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">esm.data.Alphabet</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_token_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">,</span> </span><span class="pre">torch.Tensor</span><span class="p"><span class="pre">]</span></span><a class="headerlink" href="#biotransformers.lightning_utils.data.collate_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Collate function to mask tokens.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>samples</strong> – a sequences of (label, seq).</p></li>
<li><p><strong>tokenizer</strong> – facebook tokenizer, that accepts sequences of (label, seq_str)
and outputs (labels, seq_strs, tokens).</p></li>
<li><p><strong>alphabet</strong> – facebook alphabet.</p></li>
<li><p><strong>masking_ratio</strong> – ratio of tokens to be masked.</p></li>
<li><p><strong>masking_prob</strong> – probability that the chose token is replaced with a mask token.</p></li>
<li><p><strong>random_token_prob</strong> – probability that the chose token is replaced with a random token.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>model input
targets: model target
mask_indices: indices of masked tokens</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tokens</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data._filter_sequence">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">_filter_sequence</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences_list</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_len</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><a class="headerlink" href="#biotransformers.lightning_utils.data._filter_sequence" title="Permalink to this definition">¶</a></dt>
<dd><p>Function that filter the length of a sequence list</p>
<p>Filtering depends on the type of model. It is automatically enforce as ESM1b
does’nt manage sequence longer that 1024.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequences_list</strong> – list of sequences</p></li>
<li><p><strong>model</strong> – name of the model</p></li>
<li><p><strong>length</strong> – length limit to consider</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError is model filter_len &lt; 0</strong> – </p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data.get_batch_indices">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">get_batch_indices</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequence_strs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">toks_per_batch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_toks_per_seq</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><a class="headerlink" href="#biotransformers.lightning_utils.data.get_batch_indices" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the batch idx based on the number of tokens in sequences</p>
<p>It computes a list of list of int which are the list of the indexes to consider
to build a batch.
.. rubric:: Example</p>
<p>returning [[1,3,8],[4,7,10],[11],[12]] means that the first batch  will be
composed of sequence at index 1,3,8 for the first batch, sequence 11 for the
third batch. The idea is to consider a maximum number of tokens per batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sequence_strs</strong> – list of string</p></li>
<li><p><strong>filter_len</strong> – </p></li>
<li><p><strong>toks_per_batch</strong> (<em>int</em>) – Maxi number of token per batch</p></li>
<li><p><strong>extra_toks_per_seq</strong> (<em>int</em><em>, </em><em>optional</em>) – . Defaults to 0.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of batches indexes</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>List</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="biotransformers.lightning_utils.data.create_dataloader">
<code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">create_dataloader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sequences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphabet</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#biotransformers.lightning_utils.data.AlphabetDataLoader" title="biotransformers.lightning_utils.data.AlphabetDataLoader"><span class="pre">AlphabetDataLoader</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_len</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_token_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">toks_per_batch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_toks_per_seq</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2</span></span></em><span class="sig-paren">)</span> &#x2192; <span class="pre">torch.utils.data.DataLoader</span><a class="headerlink" href="#biotransformers.lightning_utils.data.create_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Create the PyTorch Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filenames</strong> – list of sequences</p></li>
<li><p><strong>alphabet</strong> – facebook alphabet.</p></li>
<li><p><strong>filter_len</strong> – whether filter data wrt len.batch_seq</p></li>
<li><p><strong>num_workers</strong> – num of parallel data samplers</p></li>
<li><p><strong>masking_ratio</strong> – ratio of tokens to be masked.</p></li>
<li><p><strong>masking_prob</strong> – probability that the chose token is replaced with a mask token.</p></li>
<li><p><strong>random_token_prob</strong> – probability that the chose token is replaced with a random token.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>torch DataLoader</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="biotransformers.lightning_utils.data.BioDataModule">
<em class="property"><span class="pre">class</span> </em><code class="sig-prename descclassname"><span class="pre">biotransformers.lightning_utils.data.</span></code><code class="sig-name descname"><span class="pre">BioDataModule</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_sequences</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphabet</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><a class="reference internal" href="#biotransformers.lightning_utils.data.AlphabetDataLoader" title="biotransformers.lightning_utils.data.AlphabetDataLoader"><span class="pre">AlphabetDataLoader</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_len</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_ratio</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">masking_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_token_prob</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">toks_per_batch</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_toks_per_seq</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_workers</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">int</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">bool</span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BioDataModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-obj docutils literal notranslate"><span class="pre">pytorch_lightning.LightningDataModule</span></code></p>
<p>A DataModule standardizes the training, val, test splits, data preparation and transforms.
The main advantage is consistent data splits, data preparation and transforms across models.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># download, split, etc...</span>
        <span class="c1"># only called on 1 GPU/TPU in distributed</span>
    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># make assignments here (val/train/test split)</span>
        <span class="c1"># called on every process in DDP</span>
    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">train_split</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_split</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">val_split</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_split</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">test_split</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_split</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># clean up after fit or test</span>
        <span class="c1"># called on every process in DDP</span>
</pre></div>
</div>
<p>A DataModule implements 6 key methods:</p>
<ul class="simple">
<li><p><strong>prepare_data</strong> (things to do on 1 GPU/TPU not on every GPU/TPU in distributed mode).</p></li>
<li><p><strong>setup</strong>  (things to do on every accelerator in distributed mode).</p></li>
<li><p><strong>train_dataloader</strong> the training dataloader.</p></li>
<li><p><strong>val_dataloader</strong> the val dataloader(s).</p></li>
<li><p><strong>test_dataloader</strong> the test dataloader(s).</p></li>
<li><p><strong>teardown</strong> (things to do on every accelerator in distributed mode when finished)</p></li>
</ul>
<p>This allows you to share a full dataset without explaining how to download,
split transform and process the data</p>
<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BioDataModule.prepare_data">
<code class="sig-name descname"><span class="pre">prepare_data</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Use this to download and prepare data.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <cite>setup</cite> instead)
since this is NOT called on every GPU in DDP/TPU</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="n">Trainer</span><span class="p">(</span><span class="n">prepare_data_per_node</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">ddp</span><span class="o">/</span><span class="n">tpu</span><span class="p">:</span> <span class="n">init</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BioDataModule.setup">
<code class="sig-name descname"><span class="pre">setup</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span> <span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span> <span class="o"><span class="pre">=</span></span> <span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BioDataModule.setup" title="Permalink to this definition">¶</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, and predict.
This is a good hook when you need to build models dynamically or adjust something about them.
This hook is called on every process when using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stage</strong> – either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">Load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BioDataModule.train_dataloader">
<code class="sig-name descname"><span class="pre">train_dataloader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BioDataModule.train_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or more PyTorch DataLoaders for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Either a single PyTorch <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> or a collection of these
(list, dict, nested lists and dicts). In the case of multiple dataloaders, please see
this <span class="xref std std-ref">page</span></p>
</dd>
</dl>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id1"><span class="problematic" id="id2">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="biotransformers.lightning_utils.data.BioDataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.setup" title="biotransformers.lightning_utils.data.BioDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="biotransformers.lightning_utils.data.BioDataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.setup" title="biotransformers.lightning_utils.data.BioDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.train_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># single dataloader</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># multiple dataloaders, return as list</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a list of tensors: [batch_mnist, batch_cifar]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mnist_loader</span><span class="p">,</span> <span class="n">cifar_loader</span><span class="p">]</span>

<span class="c1"># multiple dataloader, return as dict</span>
<span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mnist</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">cifar</span> <span class="o">=</span> <span class="n">CIFAR</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
    <span class="n">mnist_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="n">cifar_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">cifar</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="c1"># each batch will be a dict of tensors: {&#39;mnist&#39;: batch_mnist, &#39;cifar&#39;: batch_cifar}</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;mnist&#39;</span><span class="p">:</span> <span class="n">mnist_loader</span><span class="p">,</span> <span class="s1">&#39;cifar&#39;</span><span class="p">:</span> <span class="n">cifar_loader</span><span class="p">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BioDataModule.val_dataloader">
<code class="sig-name descname"><span class="pre">val_dataloader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BioDataModule.val_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for validation.</p>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id3"><span class="problematic" id="id4">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>It’s recommended that all data downloads and preparation happen in <a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="biotransformers.lightning_utils.data.BioDataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="biotransformers.lightning_utils.data.BioDataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.train_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.val_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.test_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Single or multiple PyTorch DataLoaders.</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a validation dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>, you don’t need to
implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple validation dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="biotransformers.lightning_utils.data.BioDataModule.test_dataloader">
<code class="sig-name descname"><span class="pre">test_dataloader</span></code><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#biotransformers.lightning_utils.data.BioDataModule.test_dataloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Implement one or multiple PyTorch DataLoaders for testing.</p>
<p>The dataloader you return will not be called every epoch unless you set
<a href="#id5"><span class="problematic" id="id6">:paramref:`~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch`</span></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="biotransformers.lightning_utils.data.BioDataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.setup" title="biotransformers.lightning_utils.data.BioDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p>…</p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.prepare_data" title="biotransformers.lightning_utils.data.BioDataModule.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.setup" title="biotransformers.lightning_utils.data.BioDataModule.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.train_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.train_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">train_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.val_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.val_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">val_dataloader()</span></code></a></p></li>
<li><p><a class="reference internal" href="#biotransformers.lightning_utils.data.BioDataModule.test_dataloader" title="biotransformers.lightning_utils.data.BioDataModule.test_dataloader"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_dataloader()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning adds the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>Single or multiple PyTorch DataLoaders.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                                    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.5</span><span class="p">,),</span> <span class="p">(</span><span class="mf">1.0</span><span class="p">,))])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">&#39;/path/to/mnist/&#39;</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
                    <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">loader</span>

<span class="c1"># can also return multiple dataloaders</span>
<span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">loader_a</span><span class="p">,</span> <span class="n">loader_b</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="n">loader_n</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need a test dataset and a <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>, you don’t need to implement
this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case where you return multiple test dataloaders, the <code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code>
will have an argument <code class="docutils literal notranslate"><span class="pre">dataloader_idx</span></code> which matches the order here.</p>
</div>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


              </div>


        <div class='prev-next-bottom'>


        </div>

        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>

          By InstaDeep<br/>

            &copy; Copyright 2021, InstaDeep.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

  <script src="../../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>


  </body>
</html>
